# AliceMultiverse Configuration Example
# Copy this to settings.yaml and customize for your setup

# Directory paths
paths:
  # Where to look for new AI-generated images
  inbox: ~/Downloads/ai-images
  
  # Where to organize images into dated/project folders  
  organized: ~/Pictures/AI-Organized

# Processing options
processing:
  # Enable AI understanding (requires API keys)
  understanding: true
  
  # Copy files instead of moving them (safer)
  copy_mode: true
  
  # Watch for new files continuously
  watch: false
  
  # Preview mode - don't actually move/copy files
  dry_run: false
  
  # Force re-analysis even if cached
  force_reindex: false

# Storage configuration
storage:
  # DuckDB search index location
  search_db: data/search.duckdb
  
  # Location registry for multi-path storage (optional)
  location_registry_db: data/locations.duckdb
  
  # Use legacy simple paths (true) or multi-path system (false)
  use_legacy_paths: true  # Set to false to enable locations below
  
  # Project directories to scan
  project_paths:
    - ~/Pictures/AI-Projects
  
  # Asset directories to index
  asset_paths:
    - ~/Pictures/AI-Organized
  
  # Multi-path storage locations (when use_legacy_paths: false)
  # Uncomment and customize these examples:
  # locations:
  #   # Primary fast SSD for recent/active work
  #   - name: "Primary SSD"
  #     type: "local"
  #     path: "~/Pictures/AI-Active"
  #     priority: 100  # Highest priority for new files
  #     rules:
  #       - max_age_days: 30  # Keep files < 30 days old
  #       - min_quality_stars: 4  # Only high quality
  #       - include_types: ["image/png", "image/jpeg"]
  #   
  #   # Secondary storage for older files
  #   - name: "Archive HDD"
  #     type: "local"
  #     path: "/Volumes/Archive/AI-Images"
  #     priority: 50
  #     rules:
  #       - min_age_days: 30  # Files > 30 days old
  #   
  #   # Cloud storage for long-term archive
  #   - name: "S3 Archive"
  #     type: "s3"
  #     path: "my-ai-archive-bucket"
  #     priority: 25
  #     config:
  #       region: "us-west-2"
  #       storage_class: "GLACIER"
  #     rules:
  #       - min_age_days: 365  # Files > 1 year old

# AI Understanding configuration
understanding:
  # Providers to use (in order of preference)
  providers:
    - google      # Free tier: 50 images/day
    - deepseek    # Cheapest: ~$0.001/image
    - anthropic   # Best quality: ~$0.01/image
    - openai      # Alternative: ~$0.005/image
  
  # Preferred provider for new images
  preferred_provider: google

# Cost management
cost_management:
  # Budget limits
  budgets:
    daily: 1.00
    weekly: 5.00
    monthly: 20.00
  
  # Alert when spending reaches percentage (0.0-1.0)
  alert_threshold: 0.8

# Pipeline configuration (optional)
pipeline:
  # Processing mode: basic (free), standard, premium
  mode: basic
  
  # Cost limits per operation
  cost_limits:
    total: 10.0
    per_image: 0.05

# Event system (optional)
events:
  # Use Redis for events (requires Redis server)
  # If false, uses file-based events in ~/.alice/events/
  use_redis: false
  
  # Redis connection settings (when use_redis: true)
  redis:
    host: localhost
    port: 6379
    db: 0

# Advanced options (usually not needed)
# enhanced_metadata: false  # Enable enhanced metadata extraction
# log_level: INFO          # Logging level: DEBUG, INFO, WARNING, ERROR